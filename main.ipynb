{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4b8f52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 1 — Rappels mathématiques et approfondissement des méthodes de classification\n",
    "\n",
    "Maxence Cabiddu </br>\n",
    "maxence.cabiddu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44ece9",
   "metadata": {},
   "source": [
    "# 1. Rappels mathématiques fondamentaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d5612",
   "metadata": {},
   "source": [
    "## 1.1 Algèbre linéaire\n",
    "\n",
    "**Concepts clés :**\n",
    "- **Vecteurs** :\n",
    "  - Liste ordonnée de nombres.\n",
    "  - Exemple : caractéristiques d'un échantillon.\n",
    "  \n",
    "  Exemple de vecteur $v$ :\n",
    "\n",
    "  $$\n",
    "  v = \n",
    "  \\begin{bmatrix}\n",
    "  170 \\\\ 65\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  où :\n",
    "  - 170 = taille en cm,\n",
    "  - 65 = poids en kg.\n",
    "\n",
    "- **Matrices** :\n",
    "  - Tableau rectangulaire de vecteurs.\n",
    "  - Plusieurs échantillons organisés en lignes/colonnes.\n",
    "\n",
    "  Exemple de matrice $A$ représentant trois individus :\n",
    "\n",
    "  $$\n",
    "  A =\n",
    "  \\begin{bmatrix}\n",
    "  170 & 65 \\\\\n",
    "  160 & 60 \\\\\n",
    "  180 & 75\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  Chaque ligne correspond à un individu : $(\\text{taille}, \\text{poids})$.\n",
    "\n",
    "---\n",
    "\n",
    "**Opérations fondamentales :**\n",
    "\n",
    "- **Produit scalaire (dot product)** :\n",
    "\n",
    "  Combine deux vecteurs $v$ et $u$ en un seul nombre réel :\n",
    "\n",
    "  $$\n",
    "  v \\cdot u = \\sum_{i=1}^n v_i u_i\n",
    "  $$\n",
    "\n",
    "  ➔ Cela mesure **à quel point deux vecteurs pointent dans la même direction**.  \n",
    "  Si le produit est élevé, les vecteurs sont \"alignés\" ; s'il est nul, ils sont perpendiculaires.\n",
    "\n",
    "\n",
    "  **Exemple :**\n",
    "\n",
    "  Supposons deux individus décrits par (taille en cm, poids en kg) :\n",
    "\n",
    "  - Individu A :\n",
    "\n",
    "    $$\n",
    "    v =\n",
    "    \\begin{bmatrix}\n",
    "    170 \\\\\n",
    "    65\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - Individu B :\n",
    "\n",
    "    $$\n",
    "    u =\n",
    "    \\begin{bmatrix}\n",
    "    180 \\\\\n",
    "    70\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  Calcul du produit scalaire :\n",
    "\n",
    "  $$\n",
    "  v \\cdot u = (170 \\times 180) + (65 \\times 70)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  v \\cdot u = 30600 + 4550 = 35150\n",
    "  $$\n",
    "\n",
    "  ➔ Le produit scalaire est élevé, ce qui signifie que **les vecteurs (caractéristiques)** sont **assez alignés** :  \n",
    "  les deux individus sont **de taille et de poids relativement similaires**.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://math.libretexts.org/@api/deki/files/1833/1.3.3.jpg?revision=1\" alt=\"Illustration produit scalaire\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "*Source : [math.libretexts.org – Illustration produit scalaire](https://math.libretexts.org)*\n",
    "\n",
    "---\n",
    "\n",
    "- **Norme** :\n",
    "\n",
    "  Donne la **longueur** d'un vecteur $ v $, c’est-à-dire sa distance à l'origine :\n",
    "\n",
    "  $$\n",
    "  \\|v\\| = \\sqrt{\\sum_{i=1}^n v_i^2}\n",
    "  $$\n",
    "\n",
    "  ➔ Permet de calculer des distances ou de comparer la taille de différents vecteurs.\n",
    "\n",
    "  <p align=\"center\">\n",
    "    <img src=\"https://images.nagwa.com/figures/explainers/563120172134/5.svg\" alt=\"Illustration vecteur 1\" width=\"400\"/>\n",
    "    <img src=\"https://si.blaisepascal.fr/wp-content/uploads/2018/11/drawit-diagram-72.png\" alt=\"Illustration vecteur 2\" width=\"400\"/>\n",
    "  </p>\n",
    "\n",
    "  *Source : [nagwa.com – Illustration vecteur 1](https://nagwa.com)* </br>\n",
    "  *Source : [blaisepascal.fr – Illustration vecteur 2](https://blaisepascal.fr)*\n",
    "\n",
    "  **Exemple :**\n",
    "\n",
    "  $$\n",
    "  v = (3, 4)\n",
    "  $$\n",
    "  $$\n",
    "  \\|v\\| = \\sqrt{3^2 + 4^2} = \\sqrt{25} = 5\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "- **Multiplication matrice-vecteur** :\n",
    "\n",
    "  Multiplie une matrice $ A $ par un vecteur $ x $ :\n",
    "\n",
    "  $$\n",
    "  y = A \\times x\n",
    "  $$\n",
    "\n",
    "  ➔ Permet d'**appliquer une transformation linéaire** à un vecteur : rotation, changement d'échelle, etc.\n",
    "\n",
    "  **Exemple :**\n",
    "\n",
    "  $$\n",
    "  A = \n",
    "  \\begin{bmatrix}\n",
    "  1 & 2 \\\\\n",
    "  3 & 4\n",
    "  \\end{bmatrix}\n",
    "  ,\\quad\n",
    "  x =\n",
    "  \\begin{bmatrix}\n",
    "  5 \\\\\n",
    "  6\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  $$\n",
    "  y = \n",
    "  \\begin{bmatrix}\n",
    "  1\\times5 + 2\\times6 \\\\\n",
    "  3\\times5 + 4\\times6\n",
    "  \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix}\n",
    "  17 \\\\\n",
    "  39\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "- **Multiplication matrice-matrice** :\n",
    "\n",
    "  Multiplie deux matrices $ A $ (de taille $ m \\times n $) et $ B $ (de taille $ n \\times p $) :\n",
    "\n",
    "  $$\n",
    "  C = A \\times B\n",
    "  $$\n",
    "  Chaque élément $ C_{ij} $ est calculé par :\n",
    "\n",
    "  $$\n",
    "  C_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}\n",
    "  $$\n",
    "\n",
    "  ➔ Permet de **combiner plusieurs transformations** successives en une seule opération.\n",
    "\n",
    "  **Exemple :**\n",
    "\n",
    "  $$\n",
    "  A = \n",
    "  \\begin{bmatrix}\n",
    "  1 & 2 \\\\\n",
    "  3 & 4\n",
    "  \\end{bmatrix}\n",
    "  ,\\quad\n",
    "  B =\n",
    "  \\begin{bmatrix}\n",
    "  7 & 8 \\\\\n",
    "  9 & 10\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  $$\n",
    "  C = A \\times B =\n",
    "  \\begin{bmatrix}\n",
    "  1\\times7 + 2\\times9 & 1\\times8 + 2\\times10 \\\\\n",
    "  3\\times7 + 4\\times9 & 3\\times8 + 4\\times10\n",
    "  \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix}\n",
    "  25 & 28 \\\\\n",
    "  57 & 64\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Illustration de la multiplication de matrices\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://tutorathomes.com/wp-content/uploads/2022/07/image-56-1024x498.png\" alt=\"Multiplication de matrices\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "*Source : [Tutorat Homes – Multiplication de matrice](https://tutorathomes.com/)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8b72c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteur v (Personne A) : [170  65]\n",
      "Vecteur u (Personne B) : [180  70]\n",
      "Produit scalaire v . u : 35150\n",
      "Norme de v (longueur du vecteur v) : 182.00274723201295\n",
      "\n",
      "Matrice A (3 personnes) :\n",
      " [[170  65]\n",
      " [160  60]\n",
      " [180  75]]\n",
      "Dimension de A:  (3, 2)\n",
      "Vecteur x : [1 2]\n",
      "Multiplication matrice-vecteur A @ x :\n",
      " [300 280 330]\n",
      "\n",
      "Matrice B :\n",
      " [[ 7  8]\n",
      " [ 9 10]]\n",
      "Dimension de B:  (2, 2)\n",
      "Multiplication matrice-matrice A[:2, :] @ B :\n",
      " [[1775 2010]\n",
      " [1660 1880]\n",
      " [1935 2190]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1775, 2010],\n",
       "       [1660, 1880],\n",
       "       [1935, 2190]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Module interactif pour tester les opérations de base (sur des individus : taille, poids)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Définir un vecteur v (exemple : caractéristiques d'une personne : taille en cm, poids en kg)\n",
    "v = np.array([170, 65])  # Personne A : 170 cm, 65 kg\n",
    "\n",
    "# Définir un autre vecteur u\n",
    "u = np.array([180, 70])  # Personne B : 180 cm, 70 kg\n",
    "\n",
    "# Définir une matrice A (ex : 3 personnes différentes)\n",
    "A = np.array([\n",
    "    [170, 65],  # Personne 1\n",
    "    [160, 60],  # Personne 2\n",
    "    [180, 75]   # Personne 3\n",
    "])\n",
    "\n",
    "# Définir un vecteur x pour tester la multiplication matrice-vecteur\n",
    "x = np.array([1, 2])\n",
    "\n",
    "# Définir une autre matrice B pour tester la multiplication matrice-matrice\n",
    "B = np.array([\n",
    "    [7, 8],\n",
    "    [9, 10]\n",
    "])\n",
    "\n",
    "# ----------------------\n",
    "# Calculs\n",
    "# ----------------------\n",
    "\n",
    "print(\"Vecteur v (Personne A) :\", v)\n",
    "print(\"Vecteur u (Personne B) :\", u)\n",
    "print(\"Produit scalaire v . u :\", np.dot(v, u))\n",
    "print(\"Norme de v (longueur du vecteur v) :\", np.linalg.norm(v))\n",
    "\n",
    "print(\"\\nMatrice A (3 personnes) :\\n\", A)\n",
    "print(\"Dimension de A: \", A.shape)\n",
    "print(\"Vecteur x :\", x)\n",
    "print(\"Multiplication matrice-vecteur A @ x :\\n\", A @ x)\n",
    "\n",
    "print(\"\\nMatrice B :\\n\", B)\n",
    "print(\"Dimension de B: \", B.shape)\n",
    "print(\"Multiplication matrice-matrice A[:2, :] @ B :\\n\", A @ B)  # attention : tailles compatibles (3,2 @ 2,2)\n",
    "# A @ B equivalent A.dot(B)\n",
    "A.dot(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85f2d6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage des vecteurs représentant les personnes dans un repère, depuis (0, 0)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définir les vecteurs des personnes (taille en cm, poids en kg)\n",
    "personnes = np.array([\n",
    "    [170, 65],  # Personne A\n",
    "    [180, 80],  # Personne B\n",
    "    [110, 30]   # Personne C\n",
    "])\n",
    "\n",
    "noms = ['Personne A', 'Personne B', 'Personne C']\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Tracer les vecteurs depuis l'origine (0, 0)\n",
    "origin = np.zeros((3, 2))\n",
    "plt.quiver(\n",
    "    origin[:, 0], origin[:, 1],\n",
    "    personnes[:, 0], personnes[:, 1],\n",
    "    angles='xy', scale_units='xy', scale=1, color=['blue', 'green', 'red']\n",
    ")\n",
    "\n",
    "# Ajouter les extrémités\n",
    "for i in range(len(personnes)):\n",
    "    plt.plot(personnes[i, 0], personnes[i, 1], 'o')\n",
    "    plt.text(personnes[i, 0] + 2, personnes[i, 1] - 5, noms[i])\n",
    "\n",
    "# Configurer les axes pour démarrer à 0\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.xlabel('Taille (cm)')\n",
    "plt.ylabel('Poids (kg)')\n",
    "plt.title('Vecteurs représentant des personnes (taille, poids)')\n",
    "plt.grid()\n",
    "plt.gca().set_aspect('equal')  # Échelle équivalente pour X et Y\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbf2ef",
   "metadata": {},
   "source": [
    "## 1.2 Probabilités et statistiques\n",
    "\n",
    "**Concepts clés :**\n",
    "\n",
    "- **Variable aléatoire** :\n",
    "  - Discrète : prend un nombre fini ou dénombrable de valeurs (ex : lancer un dé → {1,2,3,4,5,6})\n",
    "  - Continue : peut prendre toutes les valeurs d'un intervalle (ex : température mesurée en degrés)\n",
    "\n",
    "---\n",
    "\n",
    "- **Espérance** (valeur moyenne théorique) :\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\sum_x x \\, \\mathbb{P}(X=x) \\quad \\text{(discret)}\n",
    "  $$\n",
    "\n",
    "  ou\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\int_{-\\infty}^{+\\infty} x\\, f(x) \\, dx \\quad \\text{(continu)}\n",
    "  $$\n",
    "\n",
    "  ➔ L'espérance donne **la moyenne attendue** d'une variable aléatoire si l'on répète l'expérience à l'infini.\n",
    "\n",
    "  **Exemple discret** : lancer de dé équilibré\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\frac{1+2+3+4+5+6}{6} = 3.5\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "- **Variance** (dispersion autour de l'espérance) :\n",
    "\n",
    "  $$\n",
    "  \\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n",
    "  $$\n",
    "\n",
    "  ➔ La variance mesure **l'écart moyen au carré** entre les réalisations et la moyenne attendue.  \n",
    "  ➔ Plus la variance est grande, plus les valeurs de $X$ sont **dispersées** autour de $\\mathbb{E}[X]$.\n",
    "\n",
    "  **Intuition** :\n",
    "  - Faible variance : les tirages sont proches de la moyenne.\n",
    "  - Forte variance : les tirages peuvent être très éloignés de la moyenne.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f9/Comparison_standard_deviations.svg\" alt=\"Illustration variance\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "*Source : [Wikimedia - Illustration variance](https://wikimedia.org/)*\n",
    "\n",
    "  **Exemple rapide** :\n",
    "\n",
    "  Si $X$ est un dé équilibré :\n",
    "\n",
    "  $$\n",
    "  \\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n",
    "  $$\n",
    "  où\n",
    "  $$\n",
    "  \\mathbb{E}[X^2] = \\frac{1^2+2^2+3^2+4^2+5^2+6^2}{6} = \\frac{91}{6}\n",
    "  $$\n",
    "\n",
    "  donc\n",
    "\n",
    "  $$\n",
    "  \\mathrm{Var}(X) = \\frac{91}{6} - (3.5)^2 \\approx 2.916\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61526333",
   "metadata": {},
   "source": [
    "## 1.3 Optimisation\n",
    "\n",
    "**Concepts clés :**\n",
    "\n",
    "- **Fonction de coût** :\n",
    "  - Mesure **l'erreur** entre les prédictions du modèle et les vraies valeurs.\n",
    "  - Objectif : **minimiser** cette fonction.\n",
    "\n",
    "- **Convexité** :\n",
    "  - Une fonction est convexe si **n'importe quel segment entre deux points de la courbe reste au-dessus de la courbe**.\n",
    "  - Convexité $\\Rightarrow$ **minimum global unique**.\n",
    "\n",
    "- **Descente de gradient** :\n",
    "\n",
    "  $$\n",
    "  \\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta)\n",
    "  $$\n",
    "\n",
    "  - À chaque étape, on ajuste les paramètres $\\theta$ **dans la direction qui réduit** la fonction de coût $J(\\theta)$.\n",
    "  - $\\eta$ est le **taux d'apprentissage** (learning rate).\n",
    "\n",
    "---\n",
    "\n",
    "**Exemple simple :**\n",
    "\n",
    "Minimiser :\n",
    "\n",
    "$$\n",
    "f(x) = (x-3)^2\n",
    "$$\n",
    "\n",
    "- La fonction $f(x)$ est convexe (parabole tournée vers le haut).\n",
    "- Son minimum global est atteint pour $x=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9b919",
   "metadata": {},
   "source": [
    "# 2. Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84243da3",
   "metadata": {},
   "source": [
    "## 2.1 Modèle et fonction de coût"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e177f5b",
   "metadata": {},
   "source": [
    "\n",
    "**Modèle binaire (classification à 2 classes)** :\n",
    "\n",
    "On cherche à prédire une probabilité $\\hat{y}$ que l'échantillon appartienne à la classe 1 :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^T x + b)\n",
    "$$\n",
    "\n",
    "où :\n",
    "- $w$ est le vecteur de poids,\n",
    "- $x$ est le vecteur d'entrée,\n",
    "- $b$ est le biais,\n",
    "- $\\sigma(z)$ est la **fonction sigmoïde** :\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "➔ La sigmoïde \"écrase\" n'importe quel nombre réel entre $0$ et $1$ : elle transforme une somme pondérée de variables en **probabilité**.\n",
    "\n",
    "**Visualisation de la fonction sigmoïde :**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img style=\"background-color: white\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" alt=\"Fonction sigmoïde\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "*Source : [Wikipedia – Logistic Function](https://en.wikipedia.org/wiki/Logistic_function)*\n",
    "\n",
    "- L'axe horizontal montre $z$ (le score linéaire).\n",
    "- L'axe vertical montre $\\sigma(z)$ (la probabilité prédite).\n",
    "\n",
    "On voit que :\n",
    "- Quand $z$ est très grand, $\\sigma(z) \\approx 1$,\n",
    "- Quand $z$ est très petit, $\\sigma(z) \\approx 0$,\n",
    "- Et autour de $z=0$, la courbe est **la plus raide**.\n",
    "\n",
    "---\n",
    "\n",
    "**Fonction de coût (log-loss)** :\n",
    "\n",
    "On mesure **l'écart entre les prédictions $\\hat{y}^{(i)}$ et les vraies étiquettes $y^{(i)}$** avec une fonction appelée **log-loss** ou **cross-entropy binaire** :\n",
    "\n",
    "$$\n",
    "J(w,b) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "**Intuition** :\n",
    "- Si la prédiction est correcte ($\\hat{y} \\approx y$), la perte est faible.\n",
    "- Si la prédiction est fausse ($\\hat{y}$ est loin de $y$), la perte est forte.\n",
    "\n",
    "➔ L'objectif est de **minimiser cette fonction de coût**.\n",
    "\n",
    "**Remarque**\n",
    "\n",
    "Quand on calcule la log-loss pour un seul exemple :\n",
    "\n",
    "- Si la vraie étiquette $y = 1$, la loss devient :\n",
    "\n",
    "  $$\n",
    "  -\\log(\\hat{y})\n",
    "  $$\n",
    "\n",
    "- Si la vraie étiquette $y = 0$, la loss devient :\n",
    "\n",
    "  $$\n",
    "  -\\log(1-\\hat{y})\n",
    "  $$\n",
    "\n",
    "➔ La log-loss **s'adapte automatiquement** selon que l'exemple appartient à la classe $1$ ou la classe $0$.\n",
    "\n",
    "---\n",
    "\n",
    "**Exemple rapide :**\n",
    "\n",
    "- Vraie étiquette : $y=1$\n",
    "- Prédiction : $\\hat{y}=0.9$\n",
    "\n",
    "La contribution à la fonction de coût est :\n",
    "\n",
    "$$\n",
    "-\\left( 1 \\times \\log(0.9) + (1-1) \\times \\log(1-0.9) \\right) = -\\log(0.9) \\approx 0.105\n",
    "$$\n",
    "\n",
    "(valeur faible = bonne prédiction)\n",
    "\n",
    "---\n",
    "\n",
    "**Extension multiclasses :**\n",
    "\n",
    "Quand il y a plus de deux classes :\n",
    "- On utilise **softmax** à la place de la sigmoïde :\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "- Chaque sortie représente une **probabilité par classe**.\n",
    "**Exemple rapide :**  \n",
    "Si un modèle produit les scores $(2.0,\\ 1.0,\\ 0.1)$, alors après softmax :\n",
    "\n",
    "$$\n",
    "\\hat{y} \\approx (0.66,\\ 0.24,\\ 0.10)\n",
    "$$\n",
    "\n",
    "➔ Le modèle attribue 66% à la première classe, 24% à la deuxième, 10% à la troisième.\n",
    "\n",
    "- La fonction de coût devient la **cross-entropy catégorique** :\n",
    "\n",
    "$$\n",
    "J = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^C y_c^{(i)} \\log(\\hat{y}_c^{(i)})\n",
    "$$\n",
    "\n",
    "où :\n",
    "- $C$ est le nombre de classes,\n",
    "- $y_c^{(i)}$ est 1 si l'observation $i$ appartient à la classe $c$, sinon 0.\n",
    "\n",
    "---\n",
    "\n",
    "**Résumé visuel** :\n",
    "\n",
    "| Cas | Activation | Coût utilisé |\n",
    "|:---|:---|:---|\n",
    "| Binaire (2 classes) | Sigmoïde | Log-loss |\n",
    "| Multiclasses | Softmax | Cross-entropy catégorique |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5b788d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6590011388859679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.6590, 0.2424, 0.0986], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "scores = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "softmax_class1 = np.exp(2.0) / np.sum(np.exp(scores))\n",
    "print(softmax_class1)\n",
    "\n",
    "torch.softmax(torch.tensor(scores), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54592b37",
   "metadata": {},
   "source": [
    "### Pourquoi la fonction de coût doit être convexe\n",
    "\n",
    "- Une fonction **convexe** a **un seul minimum global**.\n",
    "- Cela garantit que **la descente de gradient** trouvera ce minimum sans être piégée dans un mauvais \"creux\".\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/ConvexFunction.svg/600px-ConvexFunction.svg.png\" alt=\"Illustration fonction convexe\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "*Source : [Wikipedia — Convex Function](https://en.wikipedia.org/wiki/Convex_function)*\n",
    "\n",
    "- fonction **convexe** ➔ minimum global facile à trouver,\n",
    "\n",
    "\n",
    "**Dans la régression logistique** :\n",
    "- La log-loss est **convexe** par rapport à $w$ et $b$.\n",
    "- C'est pour cela que l'optimisation par descente de gradient **fonctionne bien**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe97c9",
   "metadata": {},
   "source": [
    "### **Exemple : classification binaire avec régression logistique**\n",
    "\n",
    "On considère un modèle de régression logistique simple avec :\n",
    "\n",
    "- Vecteur de poids : $w = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$\n",
    "- Biais : $b = 0.5$\n",
    "\n",
    "On veut prédire pour l’exemple suivant :\n",
    "\n",
    "- Entrée $x = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}$\n",
    "- Vraie étiquette $y = 0$\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 1 : Calcul du score linéaire $z$\n",
    "\n",
    "$$\n",
    "z = w^T x + b = (2 \\times 1) + (-1 \\times 3) + 0.5 = 2 - 3 + 0.5 = -0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 2 : Passage par la fonction sigmoïde\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{0.5}} \\approx \\frac{1}{1 + 1.6487} \\approx 0.3775\n",
    "$$\n",
    "\n",
    "➔ Le modèle prédit $\\hat{y} \\approx 0.3775$ (probabilité d'appartenir à la classe $1$).\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 3 : Calcul de la log-loss pour cet exemple\n",
    "\n",
    "La fonction de coût pour un seul exemple est :\n",
    "\n",
    "$$\n",
    "\\text{log-loss} = -\\left( y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}) \\right)\n",
    "$$\n",
    "\n",
    "Ici, $y = 0$, donc :\n",
    "\n",
    "$$\n",
    "\\text{log-loss} = -\\log(1-\\hat{y}) = -\\log(1-0.3775) = -\\log(0.6225)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{log-loss} \\approx -(-0.4737) = 0.4737\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Résumé\n",
    "\n",
    "| Étape | Résultat |\n",
    "|:---|:---|\n",
    "| Score linéaire $z$ | $-0.5$ |\n",
    "| Probabilité prédite $\\hat{y}$ | $0.3775$ |\n",
    "| Fonction de coût (log-loss) | $0.4737$ |\n",
    "\n",
    "\n",
    "**Interprétation** :\n",
    "- Le modèle donne 37% de chances que $x$ soit de classe $1$.\n",
    "- Comme la vraie classe est $0$, la perte est **modérée** ($\\approx 0.47$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dccfa4",
   "metadata": {},
   "source": [
    "## 2.2 Dérivées, dérivées partielles et gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e80084",
   "metadata": {},
   "source": [
    "\n",
    "### Pourquoi utiliser des dérivées ?\n",
    "\n",
    "Notre objectif est de **minimiser** la fonction de coût $J(w,b)$.\n",
    "\n",
    "- **Minimiser**, c'est trouver là où la fonction atteint son **point le plus bas**.\n",
    "- En mathématiques, pour savoir comment une fonction change, on utilise **la dérivée**.\n",
    "\n",
    "La **dérivée** d'une fonction $f(x)$ au point $x$ mesure :\n",
    "\n",
    "> \"À quelle vitesse (et dans quelle direction) $f(x)$ change si je fais **un tout petit pas** autour de $x$.\"\n",
    "\n",
    "**Définition formelle** (limite) :\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "- **$h$** est un tout petit déplacement.\n",
    "- Le rapport mesure **le taux de variation**.\n",
    "\n",
    "  <p align=\"center\">\n",
    "    <img style=\"background-color: white\" src=\"https://images.schoolmouv.fr/maths-1re-cours03-img01.png\" alt=\"Dérivation\" width=\"500\"/>\n",
    "  </p>\n",
    "\n",
    "  *Source : [schoolmouv.fr – Illustration dérivée](https://www.schoolmouv.fr)*\n",
    "---\n",
    "\n",
    "**En optimisation** :\n",
    "- Si la dérivée est **positive**, la fonction monte → on veut aller **vers la gauche** (réduire $x$).\n",
    "- Si la dérivée est **négative**, la fonction descend → on veut aller **vers la droite** (augmenter $x$).\n",
    "\n",
    "**Descente de gradient** = **aller dans la direction opposée à la dérivée** pour diminuer la fonction.\n",
    "\n",
    "---\n",
    "\n",
    "### Dérivées partielles\n",
    "\n",
    "Quand la fonction dépend de **plusieurs variables** (par exemple tous les poids $w_1, w_2, \\dots, w_n$),  \n",
    "on parle de **dérivées partielles**.\n",
    "\n",
    "**Dérivée partielle** = dérivée **par rapport à une seule variable**, **en laissant les autres constantes**.\n",
    "\n",
    "Notation pour $J(w_1, w_2, w_3)$ :\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial w_1}$ : variation de $J$ quand **seul** $w_1$ change,\n",
    "- $\\frac{\\partial J}{\\partial w_2}$ : variation de $J$ quand **seul** $w_2$ change,\n",
    "- etc.\n",
    "\n",
    "**Pourquoi ?**\n",
    "- Cela nous permet de savoir **comment ajuster chaque poids individuellement** pour réduire la perte.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient\n",
    "\n",
    "Le **gradient** est simplement **le vecteur de toutes les dérivées partielles** :\n",
    "\n",
    "$$\n",
    "\\nabla_w J = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial w_1} \\\\\n",
    "\\frac{\\partial J}{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial w_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Le gradient indique **la direction dans laquelle $J$ augmente le plus vite**.\n",
    "- En descente de gradient, **on suit la direction opposée** pour **réduire** $J$.\n",
    "\n",
    "**Métaphore simple** :\n",
    "> Imagine que tu es dans une vallée en montagne avec les yeux fermés.  \n",
    "> Le **gradient** est le **vecteur qui te dit dans quelle direction la pente monte le plus fort**.  \n",
    "> Pour descendre, tu fais l'inverse.\n",
    "\n",
    "---\n",
    "\n",
    "### Résumé visuel :\n",
    "\n",
    "| Concept | Explication simple |\n",
    "|:---|:---|\n",
    "| Dérivée | Variation locale d'une fonction par rapport à son entrée |\n",
    "| Dérivée partielle | Variation locale par rapport à **une seule variable** |\n",
    "| Gradient | Ensemble des dérivées partielles = **direction de la plus forte montée** |\n",
    "| Descente de gradient | Faire des petits pas dans la direction opposée pour **réduire la perte** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c16f3",
   "metadata": {},
   "source": [
    "### **Exemple : Calcul des gradients pour un seul exemple de régression logistique**\n",
    "\n",
    "### Rappel\n",
    "\n",
    "- Poids :\n",
    "  $$\n",
    "  w = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n",
    "  $$\n",
    "- Biais :\n",
    "  $$\n",
    "  b = 0.5\n",
    "  $$\n",
    "- Entrée :\n",
    "  $$\n",
    "  x = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n",
    "  $$\n",
    "- Vraie étiquette :\n",
    "  $$\n",
    "  y = 0\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 1 : Calcul du score linéaire $z$\n",
    "\n",
    "Déjà fait précédemment :\n",
    "\n",
    "$$\n",
    "z = w^T x + b = -0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 2 : Calcul de la probabilité prédite $\\hat{y}$\n",
    "\n",
    "Déjà fait précédemment :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) \\approx 0.3775\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 3 : Calcul de l'erreur $(\\hat{y} - y)$\n",
    "\n",
    "$$\n",
    "\\hat{y} - y = 0.3775 - 0 = 0.3775\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 4 : Calcul du gradient par rapport à $w$\n",
    "\n",
    "La formule pour un seul exemple est :\n",
    "\n",
    "$$\n",
    "\\nabla_w J = (\\hat{y} - y) x\n",
    "$$\n",
    "\n",
    "Calcul :\n",
    "\n",
    "$$\n",
    "\\nabla_w J = 0.3775 \\times \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0.3775 \\\\\n",
    "1.1325\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 5 : Calcul du gradient par rapport à $b$\n",
    "\n",
    "La formule est :\n",
    "\n",
    "$$\n",
    "\\partial_b J = (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "Donc :\n",
    "\n",
    "$$\n",
    "\\partial_b J = 0.3775\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Résumé des gradients\n",
    "\n",
    "| Élément | Valeur |\n",
    "|:---|:---|\n",
    "| $\\nabla_w J$ | $\\begin{bmatrix} 0.3775 \\\\ 1.1325 \\end{bmatrix}$ |\n",
    "| $\\partial_b J$ | $0.3775$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Étape 6 : Mise à jour des paramètres (si learning rate $\\eta = 0.1$)\n",
    "\n",
    "- Mise à jour de $w$ :\n",
    "\n",
    "  $$\n",
    "  w \\leftarrow w - \\eta \\nabla_w J\n",
    "  $$\n",
    "\n",
    "  Calcul :\n",
    "\n",
    "  $$\n",
    "  w = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} - 0.1 \\times \\begin{bmatrix} 0.3775 \\\\ 1.1325 \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  2 - 0.03775 \\\\\n",
    "  -1 - 0.11325\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  1.96225 \\\\\n",
    "  -1.11325\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- Mise à jour de $b$ :\n",
    "\n",
    "  $$\n",
    "  b \\leftarrow b - \\eta \\partial_b J\n",
    "  $$\n",
    "\n",
    "  Calcul :\n",
    "\n",
    "  $$\n",
    "  b = 0.5 - 0.1 \\times 0.3775 = 0.5 - 0.03775 = 0.46225\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Résumé après mise à jour\n",
    "\n",
    "| Élément | Nouvelle valeur |\n",
    "|:---|:---|\n",
    "| $w$ | $\\begin{bmatrix} 1.96225 \\\\ -1.11325 \\end{bmatrix}$ |\n",
    "| $b$ | $0.46225$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Interprétation\n",
    "\n",
    "Après une itération de descente de gradient :\n",
    "- Les poids $w$ et le biais $b$ **ont été corrigés**,\n",
    "- Le modèle **va mieux prédire** l'étiquette $y=0$ pour cet exemple particulier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88150683",
   "metadata": {},
   "source": [
    "### Mise à jour des gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb620e",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "\n",
    "- prédire de nouveau avec le modèle\n",
    "- donner la proba\n",
    "- calculer l'erreur\n",
    "\n",
    "- calculer les gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7edd8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Paramètres après une itération\n",
    "w = np.array([1.96225, -1.11325])\n",
    "x = np.array([1.0, 3.0])\n",
    "b = 0.46225\n",
    "y = 0\n",
    "\n",
    "# Forward\n",
    "\n",
    "\n",
    "# Backward\n",
    "\n",
    "\n",
    "# Mise à jour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aeb630",
   "metadata": {},
   "source": [
    "#### corrigé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f5c18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proba d'appartenir à la classe 1 : 0.2859267273325916\n",
      "Erreur log-loss : 0.33676969912442517\n",
      "\n",
      "Gradient par rapport à w : [0.28592673 0.85778018]\n",
      "Gradient par rapport à b : 0.2859267273325916\n",
      "\n",
      "Nouveaux poids w: [ 1.93365733 -1.19902802]\n",
      "Nouveau biais b: 0.43365732726674083\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Paramètres après une itération\n",
    "w = np.array([1.96225, -1.11325])\n",
    "x = np.array([1.0, 3.0])\n",
    "b = 0.46225\n",
    "y = 0\n",
    "# Learning rate\n",
    "eta = 0.1\n",
    "\n",
    "\n",
    "# Forward\n",
    "z = w.T.dot(x) + b\n",
    "y_hat = torch.sigmoid(torch.tensor(z))\n",
    "\n",
    "print(\"Proba d'appartenir à la classe 1 :\", y_hat.item())\n",
    "print(\"Erreur log-loss :\", -torch.log(1 - y_hat).item())\n",
    "\n",
    "# Backward\n",
    "dwj = (y_hat.item() - y) * x\n",
    "dwb = y_hat.item() - y\n",
    "\n",
    "print(\"\\nGradient par rapport à w :\", dwj)\n",
    "print(\"Gradient par rapport à b :\", dwb)\n",
    "\n",
    "\n",
    "# Mise à jour\n",
    "new_w = w - eta * dwj\n",
    "new_b = b - eta * dwb\n",
    "\n",
    "print(\"\\nNouveaux poids w:\", new_w)\n",
    "print(\"Nouveau biais b:\", new_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79f170",
   "metadata": {},
   "source": [
    "## 2.3 Impact du learning rate\n",
    "\n",
    "---\n",
    "\n",
    "### Qu'est-ce que le learning rate ?\n",
    "\n",
    "- Le **learning rate** ($\\eta$) contrôle **la taille des pas** que l'on fait à chaque mise à jour des paramètres.\n",
    "- Il joue un rôle **crucial** dans l'efficacité et la stabilité de l'apprentissage.\n",
    "\n",
    "---\n",
    "\n",
    "### Effets d'un learning rate mal choisi\n",
    "\n",
    "- **Learning rate trop petit** :\n",
    "  - Les mises à jour sont **très lentes**.\n",
    "  - L'optimisation peut mettre **beaucoup de temps** à converger.\n",
    "\n",
    "- **Learning rate trop grand** :\n",
    "  - Le modèle peut **osciller** autour du minimum sans jamais s'en approcher.\n",
    "  - Peut même **diverger**, c'est-à-dire que la perte augmente au lieu de diminuer.\n",
    "\n",
    "---\n",
    "\n",
    "### Illustration schématique\n",
    "\n",
    "| Learning rate | Comportement |\n",
    "|:---|:---|\n",
    "| Petit $\\eta$ | Convergence lente |\n",
    "| Bon $\\eta$ | Convergence rapide et stable |\n",
    "| Grand $\\eta$ | Divergences, oscillations |\n",
    "\n",
    "---\n",
    "\n",
    "### Recommandation pratique\n",
    "\n",
    "- Toujours **tester plusieurs valeurs** de learning rate (ex : $0.001$, $0.01$, $0.1$, $1.0$).\n",
    "- Observer l'évolution de la **fonction de coût** au cours des itérations.\n",
    "- Éventuellement utiliser des méthodes de réglage automatique (scheduler, annealing).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fac333",
   "metadata": {},
   "source": [
    "## 2.4 Standardisation des données\n",
    "\n",
    "---\n",
    "\n",
    "### Pourquoi standardiser les données ?\n",
    "\n",
    "- La régression logistique repose sur l'optimisation du produit $w^T x$.\n",
    "- Si certaines features ont des valeurs beaucoup plus grandes que d'autres, elles peuvent **dominer l'apprentissage**.\n",
    "- Cela peut rendre la convergence **plus difficile** et **plus lente**.\n",
    "\n",
    "---\n",
    "\n",
    "### Qu'est-ce que standardiser ?\n",
    "\n",
    "Standardiser consiste à transformer chaque feature $x_j$ selon la formule :\n",
    "\n",
    "$$\n",
    "x_j' = \\frac{x_j - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "\n",
    "où :\n",
    "- $\\mu_j$ est la **moyenne** de la feature $x_j$ sur le dataset,\n",
    "- $\\sigma_j$ est **l'écart-type** de la feature $x_j$.\n",
    "\n",
    "Après standardisation :\n",
    "- Chaque feature a une moyenne $0$ et un écart-type $1$.\n",
    "\n",
    "\n",
    "  <p align=\"center\">\n",
    "    <img style=\"background-color: white\" src=\"https://pantelis.github.io/cs677/docs/common/lectures/optimization/whitening/images/preprocessing-1.jpeg#center\" alt=\"normalization\" width=\"500\"/>\n",
    "  </p>\n",
    "\n",
    "  *Source : [Standardisation des données](https://pantelis.github.io/cs677/docs/common/lectures/optimization/whitening/)*\n",
    "\n",
    "---\n",
    "\n",
    "### Effets positifs de la standardisation\n",
    "\n",
    "- Facilite **la descente de gradient** (les courbes de coût sont plus \"circulaires\" au lieu d'être allongées).\n",
    "- Permet au modèle d'**apprendre plus rapidement**.\n",
    "- Rend **l'interprétation** des poids plus cohérente.\n",
    "\n",
    "---\n",
    "\n",
    "### Quand est-ce indispensable ?\n",
    "\n",
    "- Lorsque les features sont sur **des échelles différentes** (ex : taille en cm, poids en kg, revenu en euros).\n",
    "- En pratique, il est souvent **préférable de standardiser systématiquement** pour les modèles linéaires.\n",
    "\n",
    "---\n",
    "\n",
    "### Remarque\n",
    "\n",
    "- Si une régularisation est utilisée (comme L2), la standardisation est **encore plus importante** pour éviter un biais artificiel dû à la différence d'échelle des features.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemple pratique\n",
    "\n",
    "Utilisation classique avec `scikit-learn` :\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74073a3",
   "metadata": {},
   "source": [
    "## 2.5 Régularisation (L2)\n",
    "\n",
    "---\n",
    "\n",
    "### Pourquoi ajouter de la régularisation ?\n",
    "\n",
    "- Quand le modèle est trop **complexe** (beaucoup de features), il peut **sur-apprendre** (overfitting) les détails du jeu de données.\n",
    "- La **régularisation** pénalise les poids trop grands pour **forcer le modèle à rester simple**.\n",
    "\n",
    "---\n",
    "\n",
    "### Régularisation L2 (Ridge)\n",
    "\n",
    "La régularisation L2 ajoute un terme de pénalisation à la fonction de coût :\n",
    "\n",
    "$$\n",
    "J_{\\text{total}}(w,b) = J(w,b) + \\lambda \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "où :\n",
    "- $J(w,b)$ est la fonction log-loss classique,\n",
    "- $\\|w\\|_2^2 = \\sum_j w_j^2$ est la norme $L2$ au carré (la somme des carrés des poids),\n",
    "- $\\lambda$ est un hyperparamètre qui contrôle l'intensité de la pénalisation.\n",
    "\n",
    "---\n",
    "\n",
    "### Effet de la régularisation\n",
    "\n",
    "- Encourage les **petits poids** $w_j$ (proches de zéro).\n",
    "- Réduit le **risque de sur-apprentissage**.\n",
    "- Améliore parfois la **généralisation** du modèle sur de nouvelles données.\n",
    "\n",
    "---\n",
    "\n",
    "### Formules de gradient modifiées\n",
    "\n",
    "- Pour les poids $w$ :\n",
    "\n",
    "  $$\n",
    "  \\nabla_w J_{\\text{total}} = \\nabla_w J + 2\\lambda w\n",
    "  $$\n",
    "\n",
    "- Le gradient par rapport au biais $b$ **ne change pas** car on ne pénalise pas le biais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
