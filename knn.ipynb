{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78eccf6",
   "metadata": {},
   "source": [
    "# 3. K-Nearest Neighbors (KNN)\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Principe du KNN\n",
    "\n",
    "---\n",
    "\n",
    "### Qu'est-ce que KNN ?\n",
    "\n",
    "- KNN (K-Nearest Neighbors) est une **méthode non paramétrique** de classification.\n",
    "- **Idée principale** : pour prédire la classe d'un nouvel exemple $x$ :\n",
    "  - On regarde ses $k$ voisins les plus proches dans le jeu d'entraînement,\n",
    "  - On fait voter ces $k$ voisins pour déterminer la classe de $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Illustration simple\n",
    "\n",
    "- Si $k=3$ :\n",
    "  - On cherche les 3 points les plus proches,\n",
    "  - La classe majoritaire parmi ces 3 voisins devient la classe prédite.\n",
    "\n",
    "  <p align=\"center\">\n",
    "    <img style=\"background-color: white\" src=\"https://copyassignment.com/wp-content/uploads/2022/08/Category-A-1024x1024.png\" alt=\"Dérivation\" width=\"500\"/>\n",
    "  </p>\n",
    "\n",
    "  *Source : [copyassignment.com – Illustration knn](https://copyassignment.com)*\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Distance et mesure de proximité\n",
    "\n",
    "---\n",
    "\n",
    "### Comment mesurer la \"proximité\" ?\n",
    "\n",
    "Le choix de la distance est crucial pour KNN.\n",
    "\n",
    "- **Distance euclidienne** (par défaut) :\n",
    "\n",
    "  $$\n",
    "  d(x, x') = \\sqrt{\\sum_j (x_j - x_j')^2}\n",
    "  $$\n",
    "\n",
    "- **Distance Manhattan** :\n",
    "\n",
    "  $$\n",
    "  d(x, x') = \\sum_j |x_j - x_j'|\n",
    "  $$\n",
    "\n",
    "- **Distance de Minkowski** (généralisation) :\n",
    "\n",
    "  $$\n",
    "  d(x, x') = \\left( \\sum_j |x_j - x_j'|^p \\right)^{1/p}\n",
    "  $$\n",
    "\n",
    "avec $p=2$ pour Euclidienne, $p=1$ pour Manhattan.\n",
    "\n",
    "---\n",
    "\n",
    "### Remarque importante\n",
    "\n",
    "- Si les features ont des échelles différentes, il est **essentiel de standardiser** les données.\n",
    "- Sinon, certaines variables peuvent dominer la distance.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Choix de k\n",
    "\n",
    "---\n",
    "\n",
    "### Effet du choix de $k$\n",
    "\n",
    "- **Petit $k$** (ex: 1, 3) :\n",
    "  - Modèle très sensible au bruit,\n",
    "  - Risque de **sur-apprentissage**.\n",
    "\n",
    "- **Grand $k$** (ex: 15, 20) :\n",
    "  - Modèle plus stable,\n",
    "  - Risque de **sous-apprentissage** (perte de finesse).\n",
    "\n",
    "---\n",
    "\n",
    "### Règle générale\n",
    "\n",
    "- Utiliser **validation croisée** pour choisir le meilleur $k$.\n",
    "- En pratique, $k$ impair est souvent utilisé pour éviter les égalités en classification binaire.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Vote majoritaire ou pondéré\n",
    "\n",
    "---\n",
    "\n",
    "### Méthodes de vote\n",
    "\n",
    "- **Vote simple** :\n",
    "  - Chaque voisin compte pour 1 vote.\n",
    "\n",
    "- **Vote pondéré** :\n",
    "  - Chaque voisin est pondéré selon sa distance : les voisins proches comptent plus que les lointains.\n",
    "\n",
    "Formule de pondération typique :\n",
    "\n",
    "$$\n",
    "\\text{poids} = \\frac{1}{\\text{distance}(x, x')}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Forces et limites du KNN\n",
    "\n",
    "---\n",
    "\n",
    "### Forces\n",
    "\n",
    "- **Simplicité** d'implémentation et d'interprétation.\n",
    "- **Pas besoin d'entraînement** : le \"modèle\" est simplement la base de données.\n",
    "\n",
    "---\n",
    "\n",
    "### Limites\n",
    "\n",
    "- **Coûteux en prédiction** : il faut parcourir tout le dataset pour chaque prédiction.\n",
    "- **Sensibilité au choix de la distance**.\n",
    "- **Sensibilité au bruit et aux dimensions** :\n",
    "  - Quand la dimension augmente, toutes les distances deviennent similaires (malédiction de la dimensionnalité).\n",
    "\n",
    "---\n",
    "\n",
    "## 3.6 Exemple pratique rapide avec `scikit-learn`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Génération d'un jeu de données simple\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Découpage en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Modèle KNN\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
